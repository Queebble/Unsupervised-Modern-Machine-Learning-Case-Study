{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f8xS-aFQHHoT"
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# hydrogen.csv hydrogen_small.csv kick.csv Prepare_SQuAD_tiny.ipynb tfidf_features.csv tfidf_features_small.csv tsdm.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOZET50ZMFZy"
   },
   "source": [
    "#Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yaqAnD3vZWTC",
    "outputId": "814a2819-acb6-48cf-b72d-8d774245befb"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\stevi\\OneDrive\\Desktop\\Assignment 2\\Datasets\\tfidf_features.csv')\n",
    "hydrogen = pd.read_csv(r'C:\\Users\\stevi\\OneDrive\\Desktop\\Assignment 2\\Datasets\\hydrogen.csv')\n",
    "df.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1V3D018V5Wfj"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(hydrogen['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "2V6xMIgxvO_Y",
    "outputId": "babaa45e-4626-4bc1-d638-7caf8885dff2"
   },
   "outputs": [],
   "source": [
    "# Prepare features\n",
    "X = df\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "id": "LyhUsD4Lf4Io",
    "outputId": "d20ce4b8-c665-40c1-9367-c0b5ebf7d386"
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "X_mat = X.to_numpy()\n",
    "\n",
    "dims = [10, 50, 100, 200, 400, 800, 1600, min(2500, X_mat.shape[1])]\n",
    "mean_dists = []\n",
    "\n",
    "for d in dims:\n",
    "    cols = np.random.choice(X_mat.shape[1], size=d, replace=False)\n",
    "    subset = X_mat[:, cols]\n",
    "    dist = pdist(subset, metric='euclidean')\n",
    "    mean_dists.append(np.mean(dist))\n",
    "\n",
    "plt.plot(dims, mean_dists, marker='o')\n",
    "plt.xlabel('Number of Dimensions')\n",
    "plt.ylabel('Average Euclidean Distance')\n",
    "plt.title('Curse of Dimensionality: Distance vs Dimensionality')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "kzWy0pEWxVHH",
    "outputId": "970a1482-8fe9-49a0-c8e5-3d61db9ced3c"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=4)\n",
    "X_pca = pca.fit_transform(X.values)\n",
    "X_pca = pd.DataFrame(X_pca, columns=[\"PC1\", \"PC2\",\"PC3\", \"PC4\"])\n",
    "X_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "6ktw7jG06bP9",
    "outputId": "7f80a216-fc03-41e8-a7d1-bfb521cf65ea"
   },
   "outputs": [],
   "source": [
    "X_pca.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZJk7tBLU6fWh",
    "outputId": "84db069f-ebc0-42f1-b558-6cd112d99c9c"
   },
   "outputs": [],
   "source": [
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "57aBuAoW60BC",
    "outputId": "1ae49e40-1938-4d20-96b9-0f82da9347f7"
   },
   "outputs": [],
   "source": [
    "print(\"Total explained variance with 4 components:\", sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 485
    },
    "id": "gU9KxpWzZdhr",
    "outputId": "44279c17-adc5-4b86-b546-785fbd7bee75"
   },
   "outputs": [],
   "source": [
    "# Create a range of components to test (10 to 50)\n",
    "comps_num = np.arange(1, 2500+1, 250)\n",
    "\n",
    "comps_variance = []\n",
    "for k in comps_num:\n",
    "    pca = PCA(n_components=k, random_state=10)\n",
    "    pca.fit(X.values)\n",
    "    comps_variance.append(pca.explained_variance_ratio_.sum())\n",
    "\n",
    "# Plot cumulative explained variance\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.grid(True)\n",
    "plt.plot(comps_num, comps_variance, marker='o')\n",
    "plt.xlabel('# Components')\n",
    "plt.ylabel('Total explained variance')\n",
    "plt.title('PCA cumulative explained variance vs components')\n",
    "plt.ylim(0, 1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 485
    },
    "id": "3P_iuwvU78Ma",
    "outputId": "31a9aab6-27bd-4a76-eb50-e17b3592e09a"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# Create a range of components to test (10 to 50)\n",
    "comps_num = np.arange(1, 300+1, 10)\n",
    "\n",
    "comps_variance = []\n",
    "for k in comps_num:\n",
    "    pca = PCA(n_components=k, random_state=10)\n",
    "    pca.fit(X.values)\n",
    "    comps_variance.append(pca.explained_variance_ratio_.sum())\n",
    "\n",
    "# Plot cumulative explained variance\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.grid(True)\n",
    "plt.plot(comps_num, comps_variance, marker='o')\n",
    "plt.xlabel('# Components')\n",
    "plt.ylabel('Total explained variance')\n",
    "plt.title('PCA cumulative explained variance vs components')\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 485
    },
    "id": "U2Z6mnehV454",
    "outputId": "fa41f5d3-012a-483d-df3b-0f77888e3c84"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# Create a range of components to test (10 to 50)\n",
    "comps_num = np.arange(1, 25, 1)\n",
    "\n",
    "comps_variance = []\n",
    "for k in comps_num:\n",
    "    pca = PCA(n_components=k, random_state=10)\n",
    "    pca.fit(X.values)\n",
    "    comps_variance.append(pca.explained_variance_ratio_.sum())\n",
    "\n",
    "# Plot cumulative explained variance\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.grid(True)\n",
    "plt.plot(comps_num, comps_variance, marker='o')\n",
    "plt.xlabel('# Components')\n",
    "plt.ylabel('Total explained variance')\n",
    "plt.title('PCA cumulative explained variance vs components')\n",
    "plt.ylim(0, 0.175)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "47zK51socv-R"
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=5)\n",
    "X_pca = pca.fit_transform(X.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NmcT5ti-dEU-",
    "outputId": "bb454629-40ab-48c7-d7cb-3935be02ba2b"
   },
   "outputs": [],
   "source": [
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "If8Xi4BmdNP2",
    "outputId": "084ce921-9bd6-40bb-ded0-2a19915a986a"
   },
   "outputs": [],
   "source": [
    "print(\"Total explained variance with 5 components:\", sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "94Uy9bv0wa36"
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2000)\n",
    "X_pca = pca.fit_transform(X.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 827
    },
    "id": "W0ht8e-dvlU5",
    "outputId": "5fb428c5-3dc9-4a0b-b7a4-5f8bca0b9a68"
   },
   "outputs": [],
   "source": [
    "# We can't use `plt.scatter()` directly as that only works for 2D, so first create a 3D subplot.\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(projection=\"3d\")\n",
    "# Now we can create the scatterplot.\n",
    "scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=y, s=20)\n",
    "\n",
    "ax.legend(\n",
    "    handles=scatter.legend_elements()[0],\n",
    "    labels=[\"Not hydrogen-related\", \"Hydrogen-related\"],\n",
    "    title=\"PCA Tweet Relevance\"\n",
    ")\n",
    "ax.set_title(\"PCA Visualisation of Hydrogen Tweet Dataset (3 Components)\")\n",
    "ax.set(xlabel=\"PC1\", ylabel=\"PC2\", zlabel=\"PC3\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eD-36Jv3fWRC"
   },
   "outputs": [],
   "source": [
    "pca_2000 = PCA(n_components=2000, random_state=10)\n",
    "X_pca2000 = pca_2000.fit_transform(X.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "id": "r1ZXO4Hyhzhc",
    "outputId": "b5f1efd4-26cc-4860-df79-f9d590ba3b73"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "random_state = 10\n",
    "tsne = TSNE(n_components=2, random_state=random_state)\n",
    "tsne.fit(X_pca2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L5cbeJA_j40A",
    "outputId": "73b5feb9-0849-4cc0-bca7-615758529ce3"
   },
   "outputs": [],
   "source": [
    "print(\"KL divergence for 2 components:\", tsne.kl_divergence_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JLWtfKEPhx1q",
    "outputId": "522bbe90-1b4b-46ab-ca53-5482f848a866"
   },
   "outputs": [],
   "source": [
    "# Create an array containing the numbers 1 through 3 inclusive, this will also be our X values.\n",
    "tsne_comps_num = np.arange(1, 3+1)\n",
    "tsne_comps_divergence = []\n",
    "for num_components in tsne_comps_num:\n",
    "  # Fit a PCA model to the specified number of components.\n",
    "  tsne = TSNE(n_components=num_components, random_state=random_state)\n",
    "  tsne.fit(X_pca2000)\n",
    "  tsne_comps_divergence.append(tsne.kl_divergence_)\n",
    "  print(f\"KL Divergence for {num_components}: {tsne.kl_divergence_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "al8AyIdpkHI8",
    "outputId": "10157c81-30e1-43c1-adca-e8920c30c4d9"
   },
   "outputs": [],
   "source": [
    "# Perplexity must range between 5 and 50. Use steps of 5, as computing with an interval of 1 would take too long.\n",
    "tsne_perplexity_num = np.arange(5, 50+1, 5)\n",
    "tsne_perplexity_divergence = []\n",
    "for perplexity in tsne_perplexity_num:\n",
    "  # Fit a TSNE model to the specified number of components.\n",
    "  tsne = TSNE(n_components=2, perplexity=perplexity, random_state=random_state)\n",
    "  tsne.fit(X_pca2000)\n",
    "  tsne_perplexity_divergence.append(tsne.kl_divergence_)\n",
    "  print(f\"KL Divergence for perplexity of {perplexity}: {tsne.kl_divergence_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 562
    },
    "id": "-RIi5Sr3tiAT",
    "outputId": "4ea42f45-006d-4183-9d16-5d4235e7a781"
   },
   "outputs": [],
   "source": [
    "# Add grid lines to improve visibility.\n",
    "plt.figure(1, figsize=(12, 6))\n",
    "plt.grid()\n",
    "# Plot each of the data points, using a circular marker.\n",
    "plt.plot(tsne_perplexity_num, tsne_perplexity_divergence, marker='o')\n",
    "# Set the X axis to our component range, and the Y axis limits from 0 to 1 (as variance is a ratio between 0 and 1).\n",
    "plt.xlabel(\"Perplexity\")\n",
    "plt.ylabel(\"KL Divergence\")\n",
    "plt.title(\"t-SNE Perplexity Tuning: KL Divergence vs Perplexity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hb3LsECBwrvK"
   },
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=3, perplexity=5, random_state=random_state)\n",
    "X_tsne = tsne.fit_transform(X_pca2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 827
    },
    "id": "xEsSRiXBwODB",
    "outputId": "d1e5d030-c95e-4dfa-cb1b-c495ed9ab2bb"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(projection=\"3d\")\n",
    "scatter = ax.scatter(X_tsne[:, 0], X_tsne[:, 1], X_tsne[:, 2], c=y, s=20)\n",
    "ax.legend(\n",
    "    handles=scatter.legend_elements()[0],\n",
    "    labels=[\"Not hydrogen-related\", \"Hydrogen-related\"],\n",
    "    title=\"t-SNE Tweet Relevance\"\n",
    ")\n",
    "ax.set(xlabel=\"C1\", ylabel=\"C2\", zlabel=\"C3\")\n",
    "ax.set_title(\"t-SNE Visualisation of Hydrogen Tweet Dataset (3 Components)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-pihq6OQ3e1q"
   },
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wZ8FWFmbtLPc"
   },
   "outputs": [],
   "source": [
    "# Function to inspect Qualitative variable with simple plot and basic statistics\n",
    "def inspect_qualitative_variable(df, column):\n",
    "    print(f\"\\n=== {column} ===\")\n",
    "\n",
    "    # Calculate basic statistics\n",
    "    total = len(df)                                  # Total number of records\n",
    "    missing = df[column].isnull().sum()              # Count of missing (NaN) values\n",
    "    unique_vals = df[column].nunique(dropna=True)    # Number of unique categories (excluding NaN)\n",
    "\n",
    "    # Print summary statistics\n",
    "    print(f\"Total values       : {total}\")\n",
    "    print(f\"Missing values     : {missing}\")\n",
    "    print(f\"Unique categories  : {unique_vals}\")\n",
    "    print(\"\\nValue counts:\")\n",
    "    print(df[column].value_counts(dropna=False))     # Print count of each category including NaN\n",
    "\n",
    "    # Plot count distribution as a bar chart\n",
    "    plt.figure(figsize=(8, 5))                        # Set plot size\n",
    "    sns.countplot(\n",
    "        data=df,\n",
    "        x=column,\n",
    "        hue=column,                                   # Apply color separation by the same variable\n",
    "        order=df[column].value_counts().index,        # Display bars in descending order of frequency\n",
    "        palette='pastel'                              # Use a pastel color palette\n",
    "    )\n",
    "    plt.title(f'{column} Distribution')               # Set plot title\n",
    "    plt.xlabel(column.capitalize())                   # Label x-axis with column name\n",
    "    plt.ylabel('Count')                               # Label y-axis\n",
    "    plt.xticks(rotation=45)                           # Rotate x-axis labels for readability\n",
    "    plt.tight_layout()                                # Adjust layout to prevent overlap\n",
    "    plt.show()                                        # Display the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y72ecjZdLwTe"
   },
   "outputs": [],
   "source": [
    "# Define a function to inspect a Quantitative variable in a DataFrame\n",
    "def inspect_quantitative_variable(df, column):\n",
    "    print(f\"\\n=== {column} ===\")\n",
    "\n",
    "    # Calculate basic statistics\n",
    "    total = len(df)                                  # Total number of entries\n",
    "    missing = df[column].isnull().sum()              # Count of missing values\n",
    "    negative = (df[column] < 0).sum()                # Count of negative values\n",
    "    zero = (df[column] == 0).sum()                   # Count of zero values\n",
    "    min_val = df[column].min()                       # Minimum value\n",
    "    max_val = df[column].max()                       # Maximum value\n",
    "    mean = df[column].mean()                         # Mean of the column\n",
    "    median = df[column].median()                     # Median of the column\n",
    "    std = df[column].std()                           # Standard deviation\n",
    "\n",
    "    # Print the calculated statistics\n",
    "    print(f\"Total values     : {total}\")\n",
    "    print(f\"Missing values   : {missing}\")\n",
    "    print(f\"Negative values  : {negative}\")\n",
    "    print(f\"Zero values      : {zero}\")\n",
    "    print(f\"Min              : {min_val}\")\n",
    "    print(f\"Max              : {max_val}\")\n",
    "    print(f\"Mean             : {mean:.2f}\")\n",
    "    print(f\"Median           : {median:.2f}\")\n",
    "    print(f\"Std. Deviation   : {std:.2f}\")\n",
    "\n",
    "    # Visualise the distribution of values using a histogram and boxplot\n",
    "    plt.figure(figsize=(12, 4))                      # Set up a wide figure for side-by-side plots\n",
    "\n",
    "    # Plot histogram with KDE (kernel density estimate)\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(df[column], kde=True, bins='auto', color='steelblue')  # Automatically choose bin size\n",
    "    plt.title(f'Histogram of {column}')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    # Plot boxplot to show spread and potential outliers\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.boxplot(x=df[column], color='salmon')\n",
    "    plt.title(f'Boxplot of {column}')\n",
    "    plt.xlabel(column)\n",
    "\n",
    "    # Adjust layout to prevent overlap and display the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xIvxNgFU3dgx"
   },
   "outputs": [],
   "source": [
    "# hydrogen.csv hydrogen_small.csv kick.csv Prepare_SQuAD_tiny.ipynb tfidf_features.csv tfidf_features_small.csv tsdm.csv\n",
    "kickdf = pd.read_csv(r'C:\\Users\\stevi\\OneDrive\\Desktop\\Assignment 2\\Datasets\\kick.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dyXIzf6k3wRe",
    "outputId": "8ce3b573-43c2-438c-8ba8-67200b9409e5"
   },
   "outputs": [],
   "source": [
    "kickdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HvcOvGwV32TM",
    "outputId": "a4c1d5dd-b45b-48be-ef71-6d048ed81506"
   },
   "outputs": [],
   "source": [
    "# get more information from VehOdo\n",
    "print(kickdf['VehOdo'].describe())\n",
    "print(kickdf['VehOdo'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HkamSEvHJWMT",
    "outputId": "2ea0bddd-7fb1-41a3-f5c4-d18fe9d1c6b2"
   },
   "outputs": [],
   "source": [
    "# get more information from VehOdo\n",
    "print(kickdf['MMRAcquisitionAuctionAveragePrice'].describe())\n",
    "print(kickdf['MMRAcquisitionAuctionAveragePrice'].value_counts())\n",
    "\n",
    "# why is there a 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 598
    },
    "id": "x2iKWgg0LxFP",
    "outputId": "a08aeca9-7e69-4e97-babd-866cc4f92333"
   },
   "outputs": [],
   "source": [
    "inspect_quantitative_variable(kickdf, 'MMRAcquisitionAuctionAveragePrice')\n",
    "#Zero values      : 502"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UHgu03sUMLsV",
    "outputId": "7c4f2935-d616-4a60-c66f-dc429afa7498"
   },
   "outputs": [],
   "source": [
    "# replace '0' with NaN as 0 is technically a blank entry\n",
    "kickdf['MMRAcquisitionAuctionAveragePrice'] = kickdf['MMRAcquisitionAuctionAveragePrice'].replace([0], pd.NA)\n",
    "\n",
    "# finally for this variable there are many \"0\" so I need to impute them with the median as the data is quite skewed\n",
    "kickdf['MMRAcquisitionAuctionAveragePrice'] = (\n",
    "    kickdf['MMRAcquisitionAuctionAveragePrice']\n",
    "    .fillna(round(kickdf['MMRAcquisitionAuctionAveragePrice'].median()))\n",
    "    .astype('int64')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 598
    },
    "id": "QL53B2l5Nmuy",
    "outputId": "e7a016e7-ba17-4366-c749-6cc81f369bde"
   },
   "outputs": [],
   "source": [
    "inspect_quantitative_variable(kickdf, 'MMRAcquisitionAuctionAveragePrice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QGqHDmPeJWZw",
    "outputId": "7a239ebf-fa27-4b1b-bb15-b55174b74e1f"
   },
   "outputs": [],
   "source": [
    "# get more information from VehOdo\n",
    "print(kickdf['Make'].describe())\n",
    "print(kickdf['Make'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G6ytNqk6LTR6",
    "outputId": "a4a14773-51dc-46b0-be8c-d9dff2310735"
   },
   "outputs": [],
   "source": [
    "print(kickdf['Make'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K-x-DyHKJWo9",
    "outputId": "a3cba93d-f5b6-49c6-fe50-cbc684ac142e"
   },
   "outputs": [],
   "source": [
    "# get more information from VehOdo\n",
    "print(kickdf['WarrantyCost'].describe())\n",
    "print(kickdf['WarrantyCost'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yr9ndeIPJW9N",
    "outputId": "334be885-935f-4641-96b7-e3af054c41fa"
   },
   "outputs": [],
   "source": [
    "# get more information from VehOdo\n",
    "print(kickdf['IsBadBuy'].describe())\n",
    "print(kickdf['IsBadBuy'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jahuDMc7Nr1g",
    "outputId": "02e2a779-db5e-4e9d-df4a-ecf5c96f4aef"
   },
   "outputs": [],
   "source": [
    "kickdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "spVACWlRNu5U"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# take 3 variables and drop the rest. copy the dataframe to avoid warnings later\n",
    "kickdf2 = kickdf[['VehOdo', 'MMRAcquisitionAuctionAveragePrice', 'WarrantyCost']].copy()\n",
    "# convert df2 to matrix\n",
    "X = kickdf2.to_numpy()\n",
    "# scaling\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "id": "2vgJ73dMWlGP",
    "outputId": "ed7ccf0c-345d-419f-a3dc-0e0a0a5966f0"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# list to save the clusters and cost\n",
    "clusters = []\n",
    "inertia_vals = []\n",
    "for k in range(2, 12, 1):\n",
    " # train clustering with the specified K\n",
    " model = KMeans(n_clusters=k, random_state=random_state)\n",
    " model.fit(X)\n",
    "\n",
    " # append model to cluster list\n",
    " clusters.append(model)\n",
    " inertia_vals.append(model.inertia_)\n",
    "\n",
    "# plot the inertia vs K values\n",
    "plt.plot(range(2,12,1), inertia_vals, marker='*')\n",
    "plt.title(\"Elbow Method for Optimal K\", fontsize=12)\n",
    "plt.xlabel(\"Number of Clusters (K)\", fontsize=11)\n",
    "plt.ylabel(\"Inertia\", fontsize=11)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LMlX84vdYa70",
    "outputId": "a8fb2d7b-7441-486a-ee8c-9162fddd3116"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "print(clusters[0])\n",
    "print(\"Silhouette score for k=2\", silhouette_score(X, clusters[0].predict(X)))\n",
    "print(clusters[1])\n",
    "print(\"Silhouette score for k=3\", silhouette_score(X, clusters[1].predict(X)))\n",
    "print(clusters[2])\n",
    "print(\"Silhouette score for k=4\", silhouette_score(X, clusters[2].predict(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zlUn9M7ZVIbi",
    "outputId": "13d11032-ee78-4445-cc75-be54aba8f99a"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "random_state = 10\n",
    "# set the random state. different random state seeds might result in different centroids locations\n",
    "model = KMeans(n_clusters=3, random_state=random_state)\n",
    "model.fit(X)\n",
    "# sum of intra-cluster distances\n",
    "print(\"Sum of intra-cluster distance:\", model.inertia_)\n",
    "print(\"Centroid locations:\")\n",
    "for centroid in model.cluster_centers_:\n",
    " print(centroid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 932
    },
    "id": "-btDh9WbWQLO",
    "outputId": "e14b097f-5ddc-49a0-c272-92440cb4543c"
   },
   "outputs": [],
   "source": [
    "# set a different n_clusters\n",
    "model = KMeans(n_clusters=3, random_state=random_state)\n",
    "model.fit(X)\n",
    "# sum of intra-cluster distances\n",
    "print(\"Sum of intra-cluster distance:\", model.inertia_)\n",
    "print(\"Centroid locations:\")\n",
    "for centroid in model.cluster_centers_:\n",
    " print(centroid)\n",
    "# add column for cluster ID to data\n",
    "y = model.predict(X)\n",
    "kickdf2['Cluster_ID'] = y\n",
    "# how many in each?\n",
    "print(kickdf2['Cluster_ID'].value_counts())\n",
    "# pairplot\n",
    "cluster_g = sns.pairplot(kickdf2, hue='Cluster_ID', diag_kind='hist')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "21oeVHUS2kIE"
   },
   "outputs": [],
   "source": [
    "def inspect_clusters(df, cols, clusters_to_inspect):\n",
    " # prepare the column and bin size. Increase bin size to be more specific, but 20 is more than enough\n",
    "    n_bins = 20\n",
    "\n",
    "    for cluster in clusters_to_inspect:\n",
    "        print(f\"Distribution for cluster {cluster}\")\n",
    "\n",
    "        # create subplots\n",
    "        fig, ax = plt.subplots(nrows=len(cols), figsize=(6, 10))\n",
    "        fig.suptitle(f\"Cluster {cluster}\", fontsize=14)\n",
    "\n",
    "        for j, col in enumerate(cols):\n",
    "            # create the bins\n",
    "            bins = np.linspace(df[col].min(), df[col].max(), n_bins)\n",
    "\n",
    "            # plot distribution of the cluster using histogram\n",
    "            sns.histplot(df[df['Cluster_ID'] == cluster][col],\n",
    "                         bins=bins, ax=ax[j], kde=True, stat=\"density\")\n",
    "\n",
    "            # plot the normal distribution with a black line\n",
    "            sns.kdeplot(df[col], ax=ax[j], color='k')\n",
    "\n",
    "            ax[j].set_xlabel(col)\n",
    "            ax[j].legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "BdjGgLrE4IzC",
    "outputId": "12e41622-43a3-41f4-d722-0baeb8dc4e81"
   },
   "outputs": [],
   "source": [
    "inspect_clusters(kickdf2, ['VehOdo', 'MMRAcquisitionAuctionAveragePrice', 'WarrantyCost'], [0, 1, 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PIlaTG2w9mea",
    "outputId": "7fd2635c-fd29-480d-8f26-5a0cc8382f1e"
   },
   "outputs": [],
   "source": [
    "# One-hot encode the 'make' column\n",
    "make_dummies = pd.get_dummies(kickdf['Make'], prefix='Make')\n",
    "kickdf3 = pd.concat([kickdf2, make_dummies], axis=1)\n",
    "\n",
    "kickdf3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IB8POsCd-ZyL",
    "outputId": "d25dc10e-09ba-47c6-fcc1-98b4ea82787d"
   },
   "outputs": [],
   "source": [
    "# Select only numeric columns for clustering\n",
    "X3 = kickdf3.drop(columns=['Cluster_ID'])\n",
    "\n",
    "# Standardise\n",
    "scaler = StandardScaler()\n",
    "X3_scaled = scaler.fit_transform(X3)\n",
    "X3_scaled = pd.DataFrame(X3_scaled, columns=X3.columns)\n",
    "\n",
    "\n",
    "X3_scaled.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GG2PCJCN-qa_",
    "outputId": "dc2e5ef5-3ad5-47b7-b42d-d01c6b9c3358"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "random_state = 10\n",
    "# set the random state. different random state seeds might result in different centroids locations\n",
    "model = KMeans(n_clusters=3, random_state=random_state)\n",
    "model.fit(X3_scaled)\n",
    "# sum of intra-cluster distances\n",
    "print(\"Sum of intra-cluster distance:\", model.inertia_)\n",
    "print(\"Centroid locations:\")\n",
    "for centroid in model.cluster_centers_:\n",
    " print(centroid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 758
    },
    "id": "60t-aKaW-tV8",
    "outputId": "a8ab3d44-5da0-407c-b659-8e4f7dffbc54"
   },
   "outputs": [],
   "source": [
    "# fit KMeans\n",
    "model = KMeans(n_clusters=3, random_state=random_state).fit(X3_scaled)\n",
    "\n",
    "# attach cluster IDs to kickdf3\n",
    "kickdf3['Cluster_ID_With_Make'] = model.labels_\n",
    "\n",
    "# pairplot only for the 3 continuous features\n",
    "sns.pairplot(\n",
    "    kickdf3,\n",
    "    vars=['VehOdo', 'MMRAcquisitionAuctionAveragePrice', 'WarrantyCost'],\n",
    "    hue='Cluster_ID_With_Make',\n",
    "    diag_kind='hist',\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8sNBLPuKvap"
   },
   "source": [
    "#Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LADKXp62V0ab",
    "outputId": "dada03bf-1b6f-450e-a3d0-cad0243cd470"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA:\", torch.version.cuda)\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AZDqJ0UCs1aP"
   },
   "outputs": [],
   "source": [
    "# hydrogen.csv hydrogen_small.csv kick.csv Prepare_SQuAD_tiny.ipynb tfidf_features.csv tfidf_features_small.csv tsdm.csv\n",
    "tsdmdf = pd.read_csv(r'C:\\Users\\stevi\\OneDrive\\Desktop\\Assignment 2\\Datasets\\tsdm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zFGp-HP_tRlp",
    "outputId": "18cf9d29-1330-4d2d-e471-4c4aedbc6f70"
   },
   "outputs": [],
   "source": [
    "tsdmdf.info()\n",
    "tsdmdf.isnull().sum()\n",
    "# observation date is appearing as object rather than datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "id": "cCIpH5Q73_He",
    "outputId": "132daca5-26b0-4be9-8c84-0a4487059f90"
   },
   "outputs": [],
   "source": [
    "tsdmdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7elIkhwt1oK3"
   },
   "outputs": [],
   "source": [
    "# convert OBSERVATION_DATE to datetime format\n",
    "tsdmdf['OBSERVATION_DATE'] = pd.to_datetime(tsdmdf['OBSERVATION_DATE'])\n",
    "\n",
    "# sort by date\n",
    "tsdmdf = tsdmdf.sort_values(['PADDOCK_ID', 'OBSERVATION_DATE']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 854
    },
    "id": "VThjeJ_k1WIF",
    "outputId": "43fad017-324e-408e-bec8-3c2e79ca3be8"
   },
   "outputs": [],
   "source": [
    "inspect_qualitative_variable(tsdmdf, 'OBSERVATION_DATE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 598
    },
    "id": "UkWzDDqH0BNy",
    "outputId": "ecb69c84-5d6e-4920-ee2f-f3e10b43b49c"
   },
   "outputs": [],
   "source": [
    "inspect_quantitative_variable(tsdmdf, 'TSDM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 598
    },
    "id": "YGVF1r150IrX",
    "outputId": "d80c6f9c-8ee1-44b7-a5ed-66777fb774ca"
   },
   "outputs": [],
   "source": [
    "inspect_quantitative_variable(tsdmdf, '15D_AVG_DAILY_RAIN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 598
    },
    "id": "lZ77QPDH0OAk",
    "outputId": "7a7aa3c2-bcb5-4cf9-ee6e-61a0582ce44c"
   },
   "outputs": [],
   "source": [
    "inspect_quantitative_variable(tsdmdf, '15D_AVG_MIN_TEMP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 598
    },
    "id": "zwfkKIvA0UKY",
    "outputId": "fdbeff88-8110-4867-b461-4f35cb079d94"
   },
   "outputs": [],
   "source": [
    "inspect_quantitative_variable(tsdmdf, '15D_AVG_RH_TMAX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 598
    },
    "id": "NYCCBMTO0TNQ",
    "outputId": "a70a06ba-a067-4ea4-a540-0bc517f30c90"
   },
   "outputs": [],
   "source": [
    "inspect_quantitative_variable(tsdmdf, '15D_AVG_RH_TMIN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 598
    },
    "id": "rf8Nl0zk0VgB",
    "outputId": "4b1d43bc-332e-4519-c887-79b941615eaf"
   },
   "outputs": [],
   "source": [
    "inspect_quantitative_variable(tsdmdf, '15D_AVG_EVAP_SYN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 598
    },
    "id": "ExuAZDBJ0Vr3",
    "outputId": "0c87c106-cff7-4a83-9953-cf56c774d36b"
   },
   "outputs": [],
   "source": [
    "inspect_quantitative_variable(tsdmdf, '15D_AVG_RADIATION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "id": "-dHqeoP-35g0",
    "outputId": "c2e32855-936e-4f8d-8f3c-d6e783ddb213"
   },
   "outputs": [],
   "source": [
    "tsdmdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y_TvDczGWZPj",
    "outputId": "3b9b7593-d15c-468a-e034-7ec31cec497e"
   },
   "outputs": [],
   "source": [
    "seq_counts = tsdmdf.groupby('PADDOCK_ID').size()\n",
    "print(seq_counts.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3KpKWA7_XyCu"
   },
   "outputs": [],
   "source": [
    "def create_sequences(sequence, lookback, forecast_horizon, target_col):\n",
    "    T, num_features = sequence.shape\n",
    "    X, y, lengths = [], [], []\n",
    "    pad_vector = np.zeros((lookback, num_features))\n",
    "\n",
    "    # Fixed-length lookback with pre-padding\n",
    "    for t in range(1, T - forecast_horizon + 1):\n",
    "        context = sequence[:t]\n",
    "        if len(context) > lookback:\n",
    "            context = context[-lookback:]\n",
    "\n",
    "        padded_context = pad_vector.copy()\n",
    "        padded_context[-len(context):] = context\n",
    "\n",
    "        X.append(padded_context)\n",
    "        y.append(sequence[t:t + forecast_horizon, target_col])\n",
    "        lengths.append(min(len(context), lookback))\n",
    "\n",
    "    return np.array(X), np.array(y), lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Q7RM2U4YmPE"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def data_prep(df, feature_columns, lookback, test_steps, target_col, forecast_horizon):\n",
    "    X_all, y_all = [], []\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    lengths_all = []\n",
    "\n",
    "    all_train_values = []\n",
    "    for _, group in df.groupby(\"PADDOCK_ID\"):\n",
    "        feature_values = group[feature_columns].values\n",
    "        if len(feature_values) > lookback + test_steps:\n",
    "            all_train_values.append(feature_values[:-test_steps])\n",
    "\n",
    "    all_train_values = np.vstack(all_train_values)\n",
    "    global_scaler = MinMaxScaler()\n",
    "    global_scaler.fit(all_train_values)\n",
    "\n",
    "    for paddock_id, group in df.groupby('PADDOCK_ID'):\n",
    "        feature_values = group[feature_columns].values\n",
    "\n",
    "        if len(feature_values) <= 194:\n",
    "            continue\n",
    "\n",
    "        train_sample = global_scaler.transform(feature_values[:-test_steps])\n",
    "        test_sample  = global_scaler.transform(feature_values[-test_steps:])\n",
    "\n",
    "        train_data.append((paddock_id, train_sample))\n",
    "        test_data.append((paddock_id, test_sample, global_scaler))\n",
    "\n",
    "        X_location, y_location, lengths = create_sequences(\n",
    "            train_sample, lookback, forecast_horizon, target_col\n",
    "        )\n",
    "\n",
    "        X_all.append(X_location)\n",
    "        y_all.append(y_location)\n",
    "        lengths_all.append(lengths)\n",
    "\n",
    "    X_all = np.concatenate(X_all, axis=0)\n",
    "    y_all = np.concatenate(y_all, axis=0)\n",
    "    lengths_all = np.concatenate(lengths_all, axis=0)\n",
    "\n",
    "    X_all = X_all.reshape((X_all.shape[0], X_all.shape[1], X_all.shape[2]))\n",
    "\n",
    "    return (\n",
    "        torch.Tensor(X_all),\n",
    "        torch.Tensor(y_all),\n",
    "        torch.Tensor(lengths_all),\n",
    "        train_data,\n",
    "        test_data,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZmUlA_9lbr8q",
    "outputId": "a30628f1-cc21-42c9-e6cb-379b1d41648c"
   },
   "outputs": [],
   "source": [
    "lookback   = 5\n",
    "forecast_horizon = 5\n",
    "test_steps = 5\n",
    "feature_columns = ['TSDM']  # univariate\n",
    "target_col = feature_columns.index('TSDM')\n",
    "\n",
    "X_5, y_5, lengths_5, train_d_5, test_d_5 = data_prep(\n",
    "    tsdmdf, feature_columns, lookback, test_steps, target_col, forecast_horizon\n",
    ")\n",
    "\n",
    "print(\"Shape of input data after sequence creation:\", X_5.shape)\n",
    "print(\"Shape of targets after sequence creation:\", y_5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BLvOAZzwhVOc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class MyLSTMNet(nn.Module):\n",
    "    def __init__(self, num_features, hidden_layer_size, num_layers, output_size, dropout_prob):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=num_features,\n",
    "            hidden_size=hidden_layer_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_layer_size, output_size)\n",
    "\n",
    "    def forward(self, data, lengths):\n",
    "        \"\"\"\n",
    "        Forward pass for variable-length LSTM input.\n",
    "        - data: tensor [batch, time, features]\n",
    "        - lengths: tensor of sequence lengths (for packing)\n",
    "        \"\"\"\n",
    "        # Pack the input sequence so padded timesteps are ignored\n",
    "        packed_data = pack_padded_sequence(\n",
    "            data, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "\n",
    "        # Pass through LSTM\n",
    "        packed_output, (hn, cn) = self.lstm(packed_data)\n",
    "\n",
    "        # Take the hidden state from the last layer (num_layers - 1)\n",
    "        last_hidden = hn[-1]  # shape: [batch, hidden_layer_size]\n",
    "\n",
    "        # Dropout + final fully connected layer to produce output\n",
    "        out = self.dropout(last_hidden)\n",
    "        out = self.fc(out)    # shape: [batch, output_size]\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bc_y4q-hkd0g"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "def train_predict_model(model, n_epochs, lr, X_all, y_all, lengths, validation_split, batch_size):\n",
    "    # Split data into train and validation sets\n",
    "    dataset = TensorDataset(X_all, y_all, lengths)\n",
    "    val_size = int(len(dataset) * validation_split)\n",
    "    train_size = len(dataset) - val_size\n",
    "    train_set, val_set = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    print(f\"The model has {sum(p.numel() for p in model.parameters() if p.requires_grad)} trainable parameters\")\n",
    "\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        for X_batch, y_batch, lengths_batch in train_loader:\n",
    "            y_pred = model(X_batch, lengths_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation check every 100 epochs\n",
    "        if epoch % 100 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                train_preds = model(X_all[train_set.indices], lengths[train_set.indices])\n",
    "                train_loss = loss_fn(train_preds, y_all[train_set.indices]).item()\n",
    "\n",
    "                val_preds = model(X_all[val_set.indices], lengths[val_set.indices])\n",
    "                val_loss = loss_fn(val_preds, y_all[val_set.indices]).item()\n",
    "\n",
    "                print(f\"Epoch {epoch+1}: train loss {train_loss:.4f}, val loss {val_loss:.4f}\")\n",
    "\n",
    "                train_loss_history.append(train_loss)\n",
    "                val_loss_history.append(val_loss)\n",
    "\n",
    "                # Save best model\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    best_model_state = model.state_dict()\n",
    "\n",
    "    # Restore best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    return train_loss_history, val_loss_history, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TEvslTLOZdbf",
    "outputId": "34d01efe-6bd5-4428-dffe-f35fdec75ec4"
   },
   "outputs": [],
   "source": [
    "import itertools, torch\n",
    "\n",
    "# Parameter grid\n",
    "hidden_sizes = [10, 20]\n",
    "num_layers = [1, 2]\n",
    "dropouts = [0.1, 0.2]\n",
    "lrs = [0.001]\n",
    "batch_sizes = [32, 64]\n",
    "\n",
    "n_epochs = 200\n",
    "validation_split = 0.2\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_cfg = None\n",
    "\n",
    "for hs, nl, do, lr, bs in itertools.product(hidden_sizes, num_layers, dropouts, lrs, batch_sizes):\n",
    "    model = MyLSTMNet(\n",
    "        num_features=X_5.shape[2],\n",
    "        hidden_layer_size=hs,\n",
    "        num_layers=nl,\n",
    "        output_size=5,\n",
    "        dropout_prob=do\n",
    "    )\n",
    "\n",
    "    train_hist, val_hist, _ = train_predict_model(\n",
    "        model, n_epochs, lr, X_5, y_5, lengths_5, validation_split, bs\n",
    "    )\n",
    "\n",
    "    val_loss = val_hist[-1] if val_hist else float('inf')\n",
    "    print(f\"hs={hs}, nl={nl}, do={do}, lr={lr}, bs={bs} -> val_loss={val_loss:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_cfg = dict(hidden_size=hs, num_layers=nl, dropout=do, lr=lr, batch_size=bs)\n",
    "\n",
    "print(\"Best parameters:\", best_cfg, \"with val_loss:\", best_val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t_RgGt-wlXAs",
    "outputId": "700c1fe3-bb0c-4c72-fc0d-ec9e00e8c8e8"
   },
   "outputs": [],
   "source": [
    "# Optimal hyperparameters (from tuning)\n",
    "hidden_layer_size = 10\n",
    "num_layers = 2\n",
    "dropout_prob = 0.2\n",
    "lr = 0.001\n",
    "batch_size = 32\n",
    "n_epochs = 501\n",
    "validation_split = 0.2\n",
    "\n",
    "# Rebuild and train\n",
    "num_features = X_5.shape[2]\n",
    "output_size = 5\n",
    "\n",
    "model_lstm_5 = MyLSTMNet(\n",
    "    num_features=num_features,\n",
    "    hidden_layer_size=hidden_layer_size,\n",
    "    num_layers=num_layers,\n",
    "    output_size=output_size,\n",
    "    dropout_prob=dropout_prob\n",
    ")\n",
    "train_loss_history_5, val_loss_history_5, model_lstm_5 = train_predict_model(\n",
    "    model_lstm_5,\n",
    "    n_epochs,\n",
    "    lr,\n",
    "    X_5,\n",
    "    y_5,\n",
    "    lengths_5,\n",
    "    validation_split,\n",
    "    batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5wZuwn-CmMbz"
   },
   "outputs": [],
   "source": [
    "def vis_train_loss(train_loss_history, val_loss_history):\n",
    " epochs = range(0, n_epochs, 100)\n",
    " plt.plot(epochs, train_loss_history, label='Training Loss')\n",
    " plt.plot(epochs, val_loss_history, label='Validation Loss') # <-- add this\n",
    " plt.xlabel('Epochs')\n",
    " plt.ylabel('Loss')\n",
    " plt.title('Loss Convergence')\n",
    " plt.legend()\n",
    " plt.grid()\n",
    " plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "id": "UdnmHsodmOUJ",
    "outputId": "5110c34f-80cf-4a92-dcc2-769bcd328dff"
   },
   "outputs": [],
   "source": [
    "vis_train_loss(train_loss_history_5, val_loss_history_5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oLtymMsJmcXp"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import root_mean_squared_error, r2_score\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def pred_eval(model, X, y, lengths, train_d, test_d, lookback, target_col):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Training metrics\n",
    "        train_preds = model(X, lengths)\n",
    "        print(\"Training RMSE:\", root_mean_squared_error(y.flatten().tolist(),\n",
    "                                                       train_preds.flatten().tolist()))\n",
    "        print(\"Training R2:\", r2_score(y.flatten().tolist(),\n",
    "                                       train_preds.flatten().tolist()))\n",
    "\n",
    "\n",
    "        X_test = []\n",
    "        y_test = []\n",
    "        lengths_test = []\n",
    "        for count, (location_id, test_values, scaler) in enumerate(test_d):\n",
    "            train_values = train_d[count][1]\n",
    "            X_test.append(train_values[-lookback:])\n",
    "            y_test.append(test_values[:, target_col])\n",
    "            lengths_test.append(len(train_values[-lookback:]))\n",
    "\n",
    "        X_test = torch.Tensor(np.array(X_test))\n",
    "        y_test = torch.Tensor(np.array(y_test))\n",
    "        lengths_test = torch.Tensor(lengths_test).long()\n",
    "\n",
    "        # Predictions & test metrics\n",
    "        test_preds = model(X_test, lengths_test)\n",
    "        print(\"Test RMSE:\", root_mean_squared_error(y_test.flatten().tolist(),\n",
    "                                                    test_preds.flatten().tolist()))\n",
    "        print(\"Test R2:\", r2_score(y_test.flatten().tolist(),\n",
    "                                   test_preds.flatten().tolist()))\n",
    "\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(y_test.flatten().tolist(), label=\"Expected Value\")\n",
    "        plt.plot(test_preds.flatten().tolist(), label=\"Predicted Value\")\n",
    "        plt.grid()\n",
    "        plt.legend(fontsize=10)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 675
    },
    "id": "LdZI0X2WnHfJ",
    "outputId": "85b1d210-8da2-434c-a3f1-f6bf89a06b92"
   },
   "outputs": [],
   "source": [
    "lookback = 5\n",
    "target_col = 0\n",
    "pred_eval(model_lstm_5, X_5, y_5, lengths_5, train_d_5, test_d_5, lookback, target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xKWq-Dak_pyO",
    "outputId": "803f7798-175c-43e2-ec4e-8efcf15663b6"
   },
   "outputs": [],
   "source": [
    "lookback   = 10\n",
    "forecast_horizon = 5\n",
    "test_steps = 5\n",
    "feature_columns = ['TSDM']  # univariate\n",
    "target_col = feature_columns.index('TSDM')\n",
    "\n",
    "X_10, y_10, lengths_10, train_d_10, test_d_10 = data_prep(\n",
    "    tsdmdf, feature_columns, lookback, test_steps, target_col, forecast_horizon\n",
    ")\n",
    "\n",
    "print(\"Shape of input data after sequence creation:\", X_10.shape)\n",
    "print(\"Shape of targets after sequence creation:\", y_10.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RQrPI1ZWAgtL",
    "outputId": "41bec445-06c7-4c92-babc-c867132caddc"
   },
   "outputs": [],
   "source": [
    "# Optimal hyperparameters (from tuning)\n",
    "hidden_layer_size = 10\n",
    "num_layers = 2\n",
    "dropout_prob = 0.2\n",
    "lr = 0.001\n",
    "batch_size = 32\n",
    "n_epochs = 501\n",
    "validation_split = 0.2\n",
    "\n",
    "# Rebuild and train\n",
    "num_features = X_10.shape[2]\n",
    "output_size = 5\n",
    "\n",
    "model_lstm_10 = MyLSTMNet(\n",
    "    num_features=num_features,\n",
    "    hidden_layer_size=hidden_layer_size,\n",
    "    num_layers=num_layers,\n",
    "    output_size=output_size,\n",
    "    dropout_prob=dropout_prob\n",
    ")\n",
    "train_loss_history_10, val_loss_history_10, model_lstm_10 = train_predict_model(\n",
    "    model_lstm_10,\n",
    "    n_epochs,\n",
    "    lr,\n",
    "    X_10,\n",
    "    y_10,\n",
    "    lengths_10,\n",
    "    validation_split,\n",
    "    batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "01Qm4k-LAjf6",
    "outputId": "39462a97-50ec-4a54-c69b-a126e51a9f21"
   },
   "outputs": [],
   "source": [
    "print(model_lstm_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l57JA3NgBDWz",
    "outputId": "6a17e00b-64d3-494f-9cbc-b6beebcd61e2"
   },
   "outputs": [],
   "source": [
    "n_epochs = 501\n",
    "lr = 0.001\n",
    "validation_split = 0.2\n",
    "batch_size = 32\n",
    "\n",
    "train_loss_history_10, val_loss_history_10, model_lstm_10 = train_predict_model(\n",
    "    model_lstm_10,\n",
    "    n_epochs,\n",
    "    lr,\n",
    "    X_10,\n",
    "    y_10,\n",
    "    lengths_10,\n",
    "    validation_split,\n",
    "    batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "id": "8bEtM9noBKAv",
    "outputId": "66f4645c-76a8-49d0-83b3-9c1b735354b3"
   },
   "outputs": [],
   "source": [
    "vis_train_loss(train_loss_history_10, val_loss_history_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "id": "6gFXNJeWBcFt",
    "outputId": "5a2a69ad-bfcb-4fe6-b47f-052161f049e2"
   },
   "outputs": [],
   "source": [
    "lookback = 10\n",
    "target_col = 0\n",
    "pred_eval(model_lstm_10, X_10, y_10, lengths_10, train_d_10, test_d_10, lookback, target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rGFmi9CeEcCH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def create_sequences_nonfixedlookback(sequence, lookback, forecast_horizon, target_col, pad_value=0.0):\n",
    "    T, num_features = sequence.shape\n",
    "    X, y, lengths = [], [], []\n",
    "\n",
    "    if lookback > 0:\n",
    "        pad_vector = np.zeros((lookback, num_features), dtype=np.float32)\n",
    "\n",
    "        for t in range(1, T - forecast_horizon + 1):\n",
    "\n",
    "            context = sequence[:t]\n",
    "            if len(context) > lookback:\n",
    "                context = context[-lookback:]\n",
    "\n",
    "\n",
    "            padded_context = pad_vector.copy()\n",
    "            padded_context[-len(context):] = context\n",
    "\n",
    "            X.append(padded_context)\n",
    "            y.append(sequence[t:t + forecast_horizon, target_col])\n",
    "            lengths.append(min(len(context), lookback))\n",
    "\n",
    "        return (np.array(X, dtype=np.float32),\n",
    "                np.array(y, dtype=np.float32),\n",
    "                np.array(lengths, dtype=np.int64))\n",
    "\n",
    "    else:\n",
    "        for t in range(1, T - forecast_horizon + 1):\n",
    "            context = torch.tensor(sequence[:t], dtype=torch.float32)\n",
    "            X.append(context)\n",
    "            y.append(torch.tensor(sequence[t:t + forecast_horizon, target_col], dtype=torch.float32))\n",
    "            lengths.append(len(context))\n",
    "\n",
    "        X_padded = pad_sequence(X, batch_first=True, padding_value=float(pad_value))\n",
    "        y_tensor = torch.stack(y)\n",
    "\n",
    "        return (X_padded.numpy(),\n",
    "                y_tensor.numpy(),\n",
    "                np.array(lengths, dtype=np.int64))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8d-h8LV0GPIh"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def data_prep_nonfixedlookback(df, feature_columns, lookback, test_steps, target_col, forecast_horizon):\n",
    "    X_all, y_all = [], []\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    lengths_all = []\n",
    "\n",
    "    all_train_values = []\n",
    "    for _, group in df.groupby(\"PADDOCK_ID\"):\n",
    "        feature_values = group[feature_columns].values\n",
    "        if len(feature_values) > lookback + test_steps:\n",
    "\n",
    "            all_train_values.append(feature_values[:-test_steps])\n",
    "\n",
    "    all_train_values = np.vstack(all_train_values)\n",
    "    global_scaler = MinMaxScaler()\n",
    "    global_scaler.fit(all_train_values)\n",
    "\n",
    "    for paddock_id, group in df.groupby('PADDOCK_ID'):\n",
    "        feature_values = group[feature_columns].values\n",
    "\n",
    "        if len(feature_values) <= 194:\n",
    "            continue\n",
    "\n",
    "        train_sample = global_scaler.transform(feature_values[:-test_steps])\n",
    "        test_sample  = global_scaler.transform(feature_values[-test_steps:])\n",
    "\n",
    "        train_data.append((paddock_id, train_sample))\n",
    "        test_data.append((paddock_id, test_sample, global_scaler))\n",
    "\n",
    "        X_location, y_location, lengths = create_sequences_nonfixedlookback(\n",
    "            train_sample, lookback, forecast_horizon, target_col\n",
    "        )\n",
    "\n",
    "        X_all.append(X_location)\n",
    "        y_all.append(y_location)\n",
    "        lengths_all.append(lengths)\n",
    "\n",
    "    X_all = np.concatenate(X_all, axis=0)\n",
    "    y_all = np.concatenate(y_all, axis=0)\n",
    "    lengths_all = np.concatenate(lengths_all, axis=0)\n",
    "\n",
    "\n",
    "    X_all = X_all.reshape((X_all.shape[0], X_all.shape[1], X_all.shape[2]))\n",
    "\n",
    "    return (\n",
    "        torch.Tensor(X_all),\n",
    "        torch.Tensor(y_all),\n",
    "        torch.Tensor(lengths_all),\n",
    "        train_data,\n",
    "        test_data,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ej6HtHmsER9o",
    "outputId": "3eb3b567-6203-4f8f-99ab-0b67281d0939"
   },
   "outputs": [],
   "source": [
    "lookback   = 0 # dont restrict\n",
    "forecast_horizon = 5\n",
    "test_steps = 5\n",
    "# multivairate (predict TSDM)\n",
    "feature_columns = ['TSDM', '15D_AVG_DAILY_RAIN','15D_AVG_MAX_TEMP',\n",
    "                   '15D_AVG_MIN_TEMP','15D_AVG_RH_TMAX','15D_AVG_RH_TMIN',\n",
    "                   '15D_AVG_EVAP_SYN','15D_AVG_RADIATION']\n",
    "\n",
    "target_col = feature_columns.index('TSDM')\n",
    "\n",
    "X_m, y_m,lengths_m, train_d_m, test_d_m = data_prep_nonfixedlookback(\n",
    "    tsdmdf, feature_columns, lookback, test_steps, target_col, forecast_horizon\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Shape of input data after sequence creation:\", X_m.shape)\n",
    "print(\"Shape of targets after sequence creation:\", y_m.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dtOfDCwTkaMc",
    "outputId": "11298f53-dc49-43d8-b8e4-8665f5acb5a5"
   },
   "outputs": [],
   "source": [
    "# Optimal hyperparameters (from tuning)\n",
    "hidden_layer_size = 10\n",
    "num_layers = 2\n",
    "dropout_prob = 0.2\n",
    "lr = 0.001\n",
    "batch_size = 32\n",
    "n_epochs = 501\n",
    "validation_split = 0.2\n",
    "\n",
    "# Rebuild and train\n",
    "num_features = X_m.shape[2]\n",
    "output_size = test_steps\n",
    "\n",
    "model_lstm_m = MyLSTMNet(\n",
    "    num_features=num_features,\n",
    "    hidden_layer_size=hidden_layer_size,\n",
    "    num_layers=num_layers,\n",
    "    output_size=output_size,\n",
    "    dropout_prob=dropout_prob\n",
    ")\n",
    "train_loss_history_m, val_loss_history_m, model_lstm_m = train_predict_model(\n",
    "    model_lstm_m,\n",
    "    n_epochs,\n",
    "    lr,\n",
    "    X_m,\n",
    "    y_m,\n",
    "    lengths_m,\n",
    "    validation_split,\n",
    "    batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "id": "M4hABQADH1Dv",
    "outputId": "59550508-d482-45b0-f2ff-3a5e93002bef"
   },
   "outputs": [],
   "source": [
    "vis_train_loss(train_loss_history_m, val_loss_history_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 675
    },
    "id": "66ATpeUNHbx8",
    "outputId": "59733991-600f-4f45-d4cc-76c5bdad0052"
   },
   "outputs": [],
   "source": [
    "target_col = 0\n",
    "pred_eval(model_lstm_m, X_m, y_m, lengths_m, train_d_m, test_d_m, lookback, target_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LrlrjLhMO0lj"
   },
   "source": [
    "#Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Lgb28LsPiQQ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import torch\n",
    "from transformers import BertweetTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import DataCollatorWithPadding, EarlyStoppingCallback\n",
    "from datasets import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# hydrogen.csv hydrogen_small.csv kick.csv Prepare_SQuAD_tiny.ipynb tfidf_features.csv tfidf_features_small.csv tsdm.csv\n",
    "df = pd.read_csv(r'C:\\Users\\stevi\\OneDrive\\Desktop\\Assignment 2\\Datasets\\hydrogen_small.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1BBCHeb5S6DW",
    "outputId": "67140f70-154c-4745-d6e6-109a0d926541"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OgFl4E9DS_zH",
    "outputId": "3d32fdf7-2067-4a01-f1e9-e888b7d8baaa"
   },
   "outputs": [],
   "source": [
    "tfidf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m9Iu_F8zTGh2",
    "outputId": "4346755c-5e87-438c-fca3-6da741531803"
   },
   "outputs": [],
   "source": [
    "df[\"label\"].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lrX2VZdP74bZ",
    "outputId": "ea0b9791-b960-455e-af9b-c9189e1d42a4"
   },
   "outputs": [],
   "source": [
    "df[\"text\"].iloc[18]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pACV6f-K9M5A"
   },
   "outputs": [],
   "source": [
    "def clean_message(text):\n",
    " # Remove URLs from the message. Match anything starting wtih \"http\", \"https\", or \"www\"\n",
    "  # followed by any non-whitespace characters (\\S).\n",
    " text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    " # Remove any tags from the message. Matches the '@' symbol followed by any letters,\n",
    " # numbers and underscores (\\w).\n",
    " text = re.sub(r\"@\\w+\", \"\", text)\n",
    " # Remove any non-ASCII characters from the text, such as emojis.\n",
    " # If you were dealing with a dataset containing non-English/Latin text, then you would\n",
    " # want to change this regular expression.\n",
    " text = re.sub(r\"[^\\x00-\\x7F]+\", \"\", text)\n",
    " # Replace any sequences of consecutive whitespace with a single space character.\n",
    " # This includes newlines and spaces.\n",
    "\n",
    " text = re.sub(r\"\\$(\\w+)\", r\"\\1\", text)\n",
    " # Normalize cashtags by removing the '$' but keeping the company name\n",
    "\n",
    " text = re.sub(r\"\\s+\", \" \", text)\n",
    " # Finally, remove any leading or trailing whitespace with the strip() function.\n",
    " return text.strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HbO9aHfN9XXx"
   },
   "outputs": [],
   "source": [
    "df[\"text\"] = df[\"text\"].apply(clean_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jMTTPYRa9ZBW",
    "outputId": "29a67a09-ff88-4f99-8cc7-5c9ed38ee4cb"
   },
   "outputs": [],
   "source": [
    "df[\"text\"].iloc[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bf7b0kSI-vMJ"
   },
   "outputs": [],
   "source": [
    "df[\"label\"] = df[\"label\"].map({\n",
    " 'Irrelevant': 0, # Negative = 0\n",
    " 'Relevant': 1 # Positive = 1\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EOv_K9wf-2mu",
    "outputId": "3d8ca566-bef0-48c1-ebb3-265ce3bc687b"
   },
   "outputs": [],
   "source": [
    "df[\"label\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "byJD3Evz-6Mz",
    "outputId": "dd61dd47-a6e6-458e-c885-14d506647a3e"
   },
   "outputs": [],
   "source": [
    "df[df[\"label\"] == 0].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "mi_6kyt0-9Ln",
    "outputId": "9b70409d-109d-4fd0-c090-b0dbeea7f980"
   },
   "outputs": [],
   "source": [
    "df[df[\"label\"] == 1].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "19ekDe-o_Aqh",
    "outputId": "ee6af67d-045c-45c0-acf0-6ad1ca130c57"
   },
   "outputs": [],
   "source": [
    "df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZEypFj8x_DEZ",
    "outputId": "60a4d69e-ee6a-4412-8a66-45b93e3663c7"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df[\"text\"].values\n",
    "y = df[\"label\"].values\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.2, random_state=random_state\n",
    ")\n",
    "print(\"Training size:\", len(X_train))\n",
    "print(\"Testing size:\", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VgH0eYc2_qDo",
    "outputId": "d8241bd3-013e-46e5-ccb2-50d1b5abe89d"
   },
   "outputs": [],
   "source": [
    "df_hydro = pd.DataFrame({\"text\": X, \"label\": y})\n",
    "df_hydro[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ETn3_lE-BO7A"
   },
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame({\"text\": X_train, \"label\": y_train})\n",
    "test_df = pd.DataFrame({\"text\": X_test, \"label\": y_test})\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "test_ds = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-KTlTUCpB7Ow",
    "outputId": "f9cb0364-5ccf-4d11-f765-43fec6e519ee"
   },
   "outputs": [],
   "source": [
    "print(\"Train dataset:\", train_ds)\n",
    "print(\"Test dataset:\", test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G-IxFNJVB9qf",
    "outputId": "4b90288b-7acd-4074-dd0c-abcc9dabaf4e"
   },
   "outputs": [],
   "source": [
    "model_name = \"vinai/bertweet-base\"\n",
    "tokenizer = BertweetTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 98,
     "referenced_widgets": [
      "82a86a90beff45309ed1b3b3200a365a",
      "15140ccae82b4745982e2cbf580cc5b7",
      "6d3863987370400d84d3adad18ae8871",
      "b767eb2f8bfd4e27b79ecbde1c50c8b1",
      "dc518c2d25864fe0a452624c756c8c5a",
      "64cdf8a4068a4c1d90b68f0623f29c0f",
      "c424930f5f8d4f7f9127e1713df8ceac",
      "9e2f58e4049c49a6aee4b071e46cfc62",
      "2986a57fcf2b4f029348c832e8a49c5e",
      "02f6c8520b13487b9ad3fcf77239bdee",
      "ccf06c838f174de3929f1337d2059cf5",
      "3dd8be411c9f4ce09f0125c15724f502",
      "12ef2167dcc7457db160539893a9868c",
      "23743b3688614102bed988157c92da82",
      "fd282cc27570409a85042036c8047ffa",
      "9b0dbe4fdb504657bb5b45921c8994b8",
      "fba90912db254dfc871228ca1dbfac47",
      "1e129810ba4340959c432d58788bd674",
      "875338e09c3347e4a91471e06b2a8d91",
      "61f3f14d0a9549fba928a91f56aaf34c",
      "27be49ccee844ca6a06288a7de8e17bf",
      "ac32a61eb61c4433bfb771633ef11e5d"
     ]
    },
    "id": "g8TBIhCWCHwT",
    "outputId": "560d27fc-87bc-418b-ecbb-4f2b4db5502c"
   },
   "outputs": [],
   "source": [
    "# Function that is applied to all samples in the dataset.\n",
    "def tokenize(batch):\n",
    " # We set truncation=True to truncate (cut off) messages that are too long.\n",
    " # NOTE: Not all models require this, you may get a warning indicating that it has no effect.\n",
    " # Padding is set to True if the model requires a fixed sequence length.\n",
    " return tokenizer(batch['text'], truncation=True, padding=True)\n",
    "# Apply to both the training and testing datasets.\n",
    "# We set batched to True which can enable parallel processing, however on my machine I found\n",
    "# it did not scale to a greater number of threads.\n",
    "train_ds = train_ds.map(tokenize, batched=True)\n",
    "test_ds = test_ds.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_3Er2QLmDxbu",
    "outputId": "8f79cbe0-9daf-4e1f-e7e9-7ac353148041"
   },
   "outputs": [],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j3wSFIJOEIKx",
    "outputId": "4ae64e25-9642-4312-8ed2-baa1c80784dd"
   },
   "outputs": [],
   "source": [
    "# Ensure the resources for any existing model has been freed.\n",
    "try:\n",
    " del model\n",
    "except NameError:\n",
    " pass\n",
    "# Download/load the base model. We use the \"vinai/bertweet-base\" model here.\n",
    "# Set the number of labels to the number of unique labels in the dataframe, which is 2.\n",
    "# Set the problem type to single label classification, since we want one class for each sample.\n",
    "modelBERTweet = RobertaForSequenceClassification.from_pretrained(\n",
    " model_name,\n",
    " num_labels=df_hydro[\"label\"].nunique(),\n",
    " problem_type=\"single_label_classification\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FbZjb9y8ENy_"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    " labels = pred.label_ids\n",
    " preds = pred.predictions.argmax(-1)\n",
    " acc = accuracy_score(labels, preds)\n",
    " prec, recall, f1, _ = precision_recall_fscore_support(\n",
    " labels, preds, average=\"binary\", pos_label=1)\n",
    " return {\n",
    " \"accuracy\": acc,\n",
    " \"precision\": prec,\n",
    " \"recall\": recall,\n",
    " \"f1\": f1\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m5NYiQAfEhue"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    " output_dir=\"./results\",\n",
    " num_train_epochs=50,\n",
    " per_device_train_batch_size=16,\n",
    " per_device_eval_batch_size=64,\n",
    " eval_strategy=\"epoch\",\n",
    " save_strategy=\"epoch\",\n",
    " learning_rate=1e-5,\n",
    " weight_decay=0.01,\n",
    " logging_dir=\"./logs\",\n",
    " logging_steps=10,\n",
    " # Added for early stopping.\n",
    " metric_for_best_model = \"loss\",\n",
    " load_best_model_at_end = True,\n",
    " seed = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267
    },
    "id": "d0b4A1r-FDUf",
    "outputId": "031e7e81-4aa0-49e1-bb17-934874fdf5fb"
   },
   "outputs": [],
   "source": [
    "# Switch the model to training mode, enabling dropout etc layers.\n",
    "modelBERTweet.train()\n",
    "trainerBERTweet = Trainer(\n",
    " model=modelBERTweet,\n",
    " args=training_args,\n",
    " train_dataset=train_ds,\n",
    " eval_dataset=test_ds,\n",
    " processing_class=tokenizer,\n",
    " data_collator=DataCollatorWithPadding(tokenizer),\n",
    " compute_metrics=compute_metrics,\n",
    " callbacks = [EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")\n",
    "trainerBERTweet.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "id": "ovQm8JnNFMHx",
    "outputId": "67b67395-8f59-47f8-d260-9865c9b76689"
   },
   "outputs": [],
   "source": [
    "# Switch the model to evaluation mode, disabling dropout etc layers.\n",
    "modelBERTweet.eval()\n",
    "# Evaluate the datasets.\n",
    "train_results = trainerBERTweet.evaluate(train_ds)\n",
    "test_results = trainerBERTweet.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "52HwSMeFFMrF",
    "outputId": "25e65f60-4c86-4adb-c12a-6407747dbce6"
   },
   "outputs": [],
   "source": [
    "def display_evaluation(setname, results):\n",
    " print(f\"{setname} Set Accuracy:\", round(results[\"eval_accuracy\"], 3))\n",
    " print(f\"{setname} Set Precision:\", round(results[\"eval_precision\"], 3))\n",
    " print(f\"{setname} Set Recall:\", round(results[\"eval_recall\"], 3))\n",
    " print(f\"{setname} Set F1 score:\", round(results[\"eval_f1\"], 3))\n",
    "display_evaluation(\"Training\", train_results)\n",
    "display_evaluation(\"Testing\", test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E6KyNpM-G8dn"
   },
   "outputs": [],
   "source": [
    "# Returns (matrix, tokens)\n",
    "def compute_attention_matrix(tokenizer, model, text):\n",
    "  # Feed into the model, you could also grab the token embedding directly\n",
    "  # from the dataset, in which case this step would be unnecessary. We want\n",
    "  # the output in Tensor format that we can feed to the model, so we use\n",
    "  # return_tensors=\"pt\" (PyTorch Tensor). Lastly, send the tensor to\n",
    "  # whichever device the model is located on. This is unnecessary if you\n",
    "  # are running purely on the CPU, but needed for models on GPUs.\n",
    "  tokens = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "  # We use torch.no_grad() to ensure the weights in the model are unchanged.\n",
    "  with torch.no_grad():\n",
    "    pred = model(**tokens, output_attentions=True)\n",
    "  # Stack layers. Depending on your model, this may have no effect.\n",
    "  # Move it back to the GPU if it was previously on the GPU.\n",
    "  attentions = torch.stack(pred.attentions).cpu()\n",
    "  # Remove the batch dimension, as there is only a zero value there.\n",
    "  attentions = attentions.squeeze(1)\n",
    "  # Average over the transformer layers and heads.\n",
    "  attentions = attentions.mean(dim=0).mean(dim=0)\n",
    "  # attentions now contains a matrix of importance from every token to every\n",
    "  # other token. e.g. if the message contained 10 tokens, it would be 10x10.\n",
    "  # Select the predicted class.\n",
    "  pred_class = pred.logits.cpu().argmax(-1).item()\n",
    "  # Also return a string representation of the tokens in the message.\n",
    "  # Plotting the integer token IDs would not be very meaningful.\n",
    "  token_strs = tokenizer.convert_ids_to_tokens(tokens[\"input_ids\"][0])\n",
    "  return (attentions, pred_class, token_strs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IrgIDj_rH1Yq"
   },
   "outputs": [],
   "source": [
    "#pred = trainer.predict(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A1ujVCMMH4jq"
   },
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensor, apply softmax, and convert back to a numpy array.\n",
    "# pred_probs_bertweet = torch.nn.functional.softmax(torch.Tensor(pred.predictions)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "0leWdQT47mUG",
    "outputId": "88b5337d-ed82-4d92-c58e-71cbafa0a035"
   },
   "outputs": [],
   "source": [
    "pred_bertweet = trainerBERTweet.predict(test_ds)\n",
    "y_test = pred_bertweet.label_ids\n",
    "\n",
    "probs_bertweet = torch.nn.functional.softmax(\n",
    "    torch.tensor(pred_bertweet.predictions), dim=1\n",
    ").numpy()\n",
    "\n",
    "auc_bertweet = roc_auc_score(y_test, probs_bertweet[:, 1])\n",
    "fpr_bertweet, tpr_bertweet, _ = roc_curve(y_test, probs_bertweet[:, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QXoG1ZeqUI5p",
    "outputId": "a1e00248-86a0-4308-8e11-90e9c9536bfc"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_true = pred_bertweet.label_ids  # true labels\n",
    "y_pred = pred_bertweet.predictions.argmax(-1)  # predicted labels\n",
    "\n",
    "print(classification_report(y_true, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zRy65shBpDAZ"
   },
   "outputs": [],
   "source": [
    "model_name = \"Twitter/twhin-bert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F9bhtUF8q2Fb",
    "outputId": "b8ccf74b-bb1b-4559-af88-8e0546790d2e"
   },
   "outputs": [],
   "source": [
    "# Ensure the resources for any existing model has been freed.\n",
    "try:\n",
    " del model\n",
    "except NameError:\n",
    " pass\n",
    "# Download/load the base model. We use the \"vinai/bertweet-base\" model here.\n",
    "# Set the number of labels to the number of unique labels in the dataframe, which is 2.\n",
    "# Set the problem type to single label classification, since we want one class for each sample.\n",
    "modelTwHIN = AutoModelForSequenceClassification.from_pretrained(\n",
    " model_name,\n",
    " num_labels=df_hydro[\"label\"].nunique(),\n",
    " problem_type=\"single_label_classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wxCzc2fRq7c2"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    " output_dir=\"./results\",\n",
    " num_train_epochs=50,\n",
    " per_device_train_batch_size=16,\n",
    " per_device_eval_batch_size=64,\n",
    " eval_strategy=\"epoch\",\n",
    " save_strategy=\"epoch\",\n",
    " learning_rate=1e-5,\n",
    " weight_decay=0.01,\n",
    " logging_dir=\"./logs\",\n",
    " logging_steps=10,\n",
    " # Added for early stopping.\n",
    " metric_for_best_model = \"loss\",\n",
    " load_best_model_at_end = True,\n",
    " seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "id": "L4_KGrTxq8sy",
    "outputId": "f77d6b67-0a91-4d4d-b20c-de16289d7a1a"
   },
   "outputs": [],
   "source": [
    "# Switch the model to training mode, enabling dropout etc layers.\n",
    "modelTwHIN.train()\n",
    "trainerTwHIN = Trainer(\n",
    " model=modelTwHIN,\n",
    " args=training_args,\n",
    " train_dataset=train_ds,\n",
    " eval_dataset=test_ds,\n",
    " processing_class=tokenizer,\n",
    " data_collator=DataCollatorWithPadding(tokenizer),\n",
    " compute_metrics=compute_metrics,\n",
    " callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "trainerTwHIN.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "id": "Mdvyxhbipi_2",
    "outputId": "ba6cd4c6-deaa-46b6-b87f-889b30e3838d"
   },
   "outputs": [],
   "source": [
    "# Switch the model to evaluation mode, disabling dropout etc layers.\n",
    "modelTwHIN.eval()\n",
    "# Evaluate the datasets.\n",
    "train_results = trainerTwHIN.evaluate(train_ds)\n",
    "test_results = trainerTwHIN.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mEkjM6iCrAy0",
    "outputId": "aa21c480-f8c4-486e-e18e-60e53adb1ac7"
   },
   "outputs": [],
   "source": [
    "def display_evaluation(setname, results):\n",
    " print(f\"{setname} Set Accuracy:\", round(results[\"eval_accuracy\"], 3))\n",
    " print(f\"{setname} Set Precision:\", round(results[\"eval_precision\"], 3))\n",
    " print(f\"{setname} Set Recall:\", round(results[\"eval_recall\"], 3))\n",
    " print(f\"{setname} Set F1 score:\", round(results[\"eval_f1\"], 3))\n",
    "display_evaluation(\"Training\", train_results)\n",
    "display_evaluation(\"Testing\", test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2kKWASKWsQqb"
   },
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensor, apply softmax, and convert back to a numpy array.\n",
    "# pred_probs_twhin = torch.nn.functional.softmax(torch.Tensor(pred.predictions)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "6LR00P-N7wtt",
    "outputId": "6bf6d05d-6e6c-4441-ec8c-5a6e37ae9d47"
   },
   "outputs": [],
   "source": [
    "pred_twhin = trainerTwHIN.predict(test_ds)\n",
    "probs_twhin = torch.nn.functional.softmax(\n",
    "    torch.tensor(pred_twhin.predictions), dim=1\n",
    ").numpy()\n",
    "\n",
    "auc_twhin = roc_auc_score(y_test, probs_twhin[:, 1])\n",
    "fpr_twhin, tpr_twhin, _ = roc_curve(y_test, probs_twhin[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Z88ixnW7013",
    "outputId": "09dee734-2a0f-4a30-d680-1d232a5f3c6b"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(\"Means:\", probs_bertweet[:,1].mean(), probs_twhin[:,1].mean())\n",
    "print(\"Arrays equal?\", np.allclose(probs_bertweet, probs_twhin))  # should be False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 485
    },
    "id": "XPf2H606sEi0",
    "outputId": "00cb4938-c65d-4011-d0e3-0ef39284d847"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(fpr_bertweet, tpr_bertweet, label=f\"BERTweet (AUC = {auc_bertweet:.3f})\", lw=1.0)\n",
    "plt.plot(fpr_twhin,    tpr_twhin,    label=f\"TwHIN-BERT (AUC = {auc_twhin:.3f})\", lw=1.0)\n",
    "plt.plot([0,1],[0,1], linestyle=\"--\", color=\"navy\", lw=0.5)\n",
    "plt.xlim([0,1]); plt.ylim([0,1])\n",
    "plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve  Hydrogen Tweet Classification\")\n",
    "plt.legend(loc=\"lower right\"); plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jpnj8SKQHQUn"
   },
   "outputs": [],
   "source": [
    "def plot_attention(attentions, tokens, title):\n",
    " # Enlarge figure to take up more of the width.\n",
    " plt.figure(figsize=(10, 8))\n",
    " plt.title(title)\n",
    " # Plot heatmap.\n",
    " sns.heatmap(\n",
    " attentions, # Plot our attention matrix.\n",
    " xticklabels=tokens, # Display token names on X axis.\n",
    " yticklabels=tokens, # Display token names on Y axis.\n",
    " cmap='binary', # Black for low, white for high\n",
    " cbar=True # Display colour bar.\n",
    " )\n",
    "\n",
    " plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A7FNIYzlHi8_"
   },
   "outputs": [],
   "source": [
    "def display_attention_matrix(tokenizer, model, text, pred_label):\n",
    "    attention, pred_class, tokens = compute_attention_matrix(tokenizer, model, text)\n",
    "    pred_label = pred_label\n",
    "    plot_attention(attention, tokens, text + f\"\\nPredicted class: {pred_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bflFUDzjBHmQ",
    "outputId": "fa47cdfe-1117-4bdc-ef03-5235792a914a"
   },
   "outputs": [],
   "source": [
    "tokenizerBT   = BertweetTokenizer.from_pretrained(\"vinai/bertweet-base\")\n",
    "tokenizerTWHIN = AutoTokenizer.from_pretrained(\"Twitter/twhin-bert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 769
    },
    "id": "evQCe0ucAdcd",
    "outputId": "5660bb60-ebd1-4e6f-c9b9-c238abbd2176"
   },
   "outputs": [],
   "source": [
    "display_attention_matrix(tokenizerBT, modelBERTweet, \"BERTweet Model: \" + df[df[\"label\"] == 1].iloc[0][\"text\"], \"Positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 758
    },
    "id": "N-URsn0MAdz8",
    "outputId": "ff7803da-beda-439a-ff20-8cc92ced6335"
   },
   "outputs": [],
   "source": [
    "display_attention_matrix(tokenizerTWHIN, modelTwHIN, \"TwHIN-BERT Model: \" + df[df[\"label\"] == 1].iloc[0][\"text\"], \"Positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 804
    },
    "id": "jR4u1ASLAWhE",
    "outputId": "aa58bfd9-18ab-4c81-bcc3-08b5d3acdeda"
   },
   "outputs": [],
   "source": [
    "display_attention_matrix(tokenizerBT, modelBERTweet, \"BERTweet Model: \" + df[df[\"label\"] == 0].iloc[0][\"text\"], \"Negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 771
    },
    "id": "TCW1t6IKHkX0",
    "outputId": "0eb4c9ed-1675-4071-d34c-706f3b9871c6"
   },
   "outputs": [],
   "source": [
    "display_attention_matrix(tokenizerTWHIN, modelTwHIN, \"TwHIN-BERT Model: \" + df[df[\"label\"] == 0].iloc[0][\"text\"], \"Negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B0wP39SWM8gb"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "tfidf = pd.read_csv(r'C:\\Users\\stevi\\OneDrive\\Desktop\\Assignment 2\\Datasets\\tfidf_features_small.csv')\n",
    "y = df_hydro['label'].astype(int).values\n",
    "X = tfidf.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BS2p-Ki7NGjz",
    "outputId": "f4057689-ddc8-4a9e-ad4a-9a0ef1ebd4c6"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "print(\"Size of training set:\", len(X_train))\n",
    "print(\"Size of testing set:\", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KWFqYUPPPSnJ",
    "outputId": "9c3b1c78-d0a6-4e60-f76e-9e65b76734be"
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(\n",
    "    solver=\"lbfgs\", #default\n",
    "    penalty=\"l2\", #default\n",
    "    C=1.0, #default\n",
    "    max_iter=100, #default\n",
    "    random_state=42\n",
    ")\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n=== Logistic Regression (TF-IDF)  baseline ===\")\n",
    "print(\"Train accuracy:\", logreg.score(X_train, y_train))\n",
    "print(\"Test  accuracy:\", logreg.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ROFfbVHIPU00",
    "outputId": "32790c8c-4f44-4a00-e06b-9f942e094065"
   },
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6eswEvBZPWUG",
    "outputId": "0b3507e1-ff5c-481e-b700-434dcb5f4d46"
   },
   "outputs": [],
   "source": [
    "# ROC-AUC\n",
    "y_prob = logreg.predict_proba(X_test)[:, 1]\n",
    "auc = roc_auc_score(y_test, y_prob)\n",
    "fpr_logreg, tpr_logreg, _ = roc_curve(y_test, y_prob)\n",
    "print(\"ROC-AUC (baseline):\", round(auc, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 485
    },
    "id": "qTYUMaeCNGv5",
    "outputId": "277c4057-e387-4b5a-8166-3d6e8cb4b3f0"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "plt.figure(figsize=(6, 5))\n",
    "\n",
    "plt.plot(fpr_bertweet, tpr_bertweet,\n",
    "         label=f\"BERTweet (AUC = {auc_bertweet:.3f})\",\n",
    "         color='red', lw=1.0)\n",
    "\n",
    "#plt.plot(fpr_twhin, tpr_twhin,\n",
    "#        label=f\"TwHIN-BERT (AUC = {auc_twhin:.3f})\",\n",
    "#        color='green', lw=1.0)\n",
    "\n",
    "plt.plot(fpr_logreg, tpr_logreg,\n",
    "         label=f\"Logistic Regression (AUC = {auc:.3f})\",\n",
    "         color='blue', lw=1.0)\n",
    "\n",
    "plt.plot([0, 1], [0, 1],\n",
    "         linestyle='--', color='navy', lw=0.5, label='Chance')\n",
    "\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve  Hydrogen Tweet Classification\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0I7wuFRPnM5z"
   },
   "source": [
    "#Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "debe47fc-6d7a-4fcb-b07f-7435895e88ce"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, DataCollatorForSeq2Seq, EarlyStoppingCallback, AutoTokenizer, AutoModelWithLMHead\n",
    "from rouge_score import rouge_scorer\n",
    "from tqdm import tqdm\n",
    "from transformers import logging as transformers_logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "28c25606-856c-4e9b-bbb3-86d0abdd798b",
    "outputId": "dfd0d7a8-de20-4c45-8343-2e244b059122"
   },
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c0437040-ad9b-4cfe-9801-d1a801a7102e"
   },
   "outputs": [],
   "source": [
    "# 1. Load and preprocess SQuAD dataset\n",
    "dataset = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "88uDH1cRonfL",
    "outputId": "ec874b2f-55f7-4420-f825-1f2b795fec5e"
   },
   "outputs": [],
   "source": [
    "print(\"Number of training examples:\", len(dataset['train']))\n",
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "42bd720f-173b-47ea-9c85-abe55b0ae8e7"
   },
   "outputs": [],
   "source": [
    "# Take subsets to avoid overload\n",
    "train_dataset = dataset[\"train\"].select(range(10000,11000))\n",
    "val_dataset = dataset[\"validation\"].select(range(3000,3100))\n",
    "test_dataset = dataset[\"validation\"].select(range(3100, 3200))  # No official SQuAD test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "726043e3",
    "outputId": "0a566f25-ba9a-499e-87cb-979b43decd8d"
   },
   "outputs": [],
   "source": [
    "print(\"Size of training set:\", len(train_dataset))\n",
    "print(\"Size of validation set:\", len(val_dataset))\n",
    "print(\"Size of testing set:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5bcfc8e4",
    "outputId": "e2ae5df4-60e6-4422-f65c-77420b7af72b"
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"t5-small\"\n",
    "#MODEL_NAME = \"t5-base\"\n",
    "MAX_INPUT_LENGTH = 512\n",
    "MAX_OUTPUT_LENGTH = 128\n",
    "# Load tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d3e2652a-32ac-4754-8bb5-3a8260021062"
   },
   "outputs": [],
   "source": [
    "def encode_question_and_context(question, context):\n",
    "    return f\"question: {question}  context: {context}\"\n",
    "\n",
    "# Obtains the context, question and answer from a given sample.\n",
    "def extract_sample_parts(sample):\n",
    "    context = sample[\"context\"]\n",
    "    question = sample[\"question\"]\n",
    "    answer = sample[\"answers\"]['text'][0]\n",
    "    question_with_context = encode_question_and_context(question, context)\n",
    "    return (question_with_context, question, answer)\n",
    "\n",
    "# Encodes the sample, returning token IDs.\n",
    "def preprocess(sample):\n",
    "    # Extract data from sample.\n",
    "    question_with_context, question, answer = extract_sample_parts(sample)\n",
    "\n",
    "    # Using truncation causes the tokenizer to emit a warning for every sample.\n",
    "    # This will generate a significant amount of messages, and likely crash\n",
    "    # your browser tab. We temporarily disable log messages to work around this.\n",
    "    # See https://github.com/huggingface/transformers/issues/14285\n",
    "    old_level = transformers_logging.get_verbosity()\n",
    "    transformers_logging.set_verbosity_error()\n",
    "\n",
    "    # Generate tokens for the input.\n",
    "    # We include both the context and the question (first two parameters).\n",
    "    input_tokens = tokenizer(question_with_context, question, padding=\"max_length\",\n",
    "                             truncation=True, max_length=MAX_INPUT_LENGTH)\n",
    "\n",
    "    # Generate tokens for the expected answer. There is no need to include the\n",
    "    output_tokens = tokenizer(answer, padding=\"max_length\", truncation=True,\n",
    "                              max_length=MAX_OUTPUT_LENGTH)\n",
    "\n",
    "    # Restore old logging level, see above.\n",
    "    transformers_logging.set_verbosity(old_level)\n",
    "\n",
    "    # The output of the tokenizer is a map containing {input_ids, attention_mask}.\n",
    "    # For trianing, we need to add the labels (answer/output tokens) to the map.\n",
    "    input_tokens[\"labels\"] = np.array(output_tokens[\"input_ids\"])\n",
    "\n",
    "    return input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "88504d94-9c04-4af6-965e-742418624148"
   },
   "outputs": [],
   "source": [
    "# Preprocess the datasets\n",
    "training_set_enc = train_dataset.map(preprocess, batched=False)\n",
    "validation_set_enc = val_dataset.map(preprocess, batched=False)\n",
    "testing_set_enc = test_dataset.map(preprocess, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dLEVvOSosuWU",
    "outputId": "be7efc08-4707-4da8-c085-9b48b26cc047"
   },
   "outputs": [],
   "source": [
    "# Prepare 20 data points for qualitative analysis\n",
    "test = testing_set_enc.select(range(20))\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aea75f9c",
    "outputId": "8572a944-57de-4dc5-d000-98f964f4d44f"
   },
   "outputs": [],
   "source": [
    "# Prepare 20 data points for qualitative analysis\n",
    "q_data = test_dataset.select(range(20))\n",
    "q_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fmIkZIxHwSuM"
   },
   "outputs": [],
   "source": [
    "# Ensure the resources for any existing model has been freed.\n",
    "try:\n",
    " del model\n",
    "except NameError:\n",
    " pass\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dQ0Rcn_OwUGl"
   },
   "outputs": [],
   "source": [
    "# We're interested in the input_ids (question/context tokens), attention mask,\n",
    "# and labels (answer token IDs). Use PyTorch tensors for all three datasets.\n",
    "columns = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "training_set_enc.set_format(type=\"torch\", columns=columns)\n",
    "validation_set_enc.set_format(type=\"torch\", columns=columns)\n",
    "testing_set_enc.set_format(type=\"torch\", columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R0aaMRHNwc7V"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    " output_dir=\"./results\",\n",
    " num_train_epochs=50,\n",
    " per_device_train_batch_size=8,\n",
    " per_device_eval_batch_size=8,\n",
    " eval_strategy=\"epoch\",\n",
    " save_strategy=\"epoch\",\n",
    " learning_rate=3e-4,\n",
    " weight_decay=0.01,\n",
    " save_total_limit=2,\n",
    " logging_dir=\"./logs\",\n",
    " logging_steps=10,\n",
    " load_best_model_at_end=True,\n",
    " metric_for_best_model=\"eval_loss\",\n",
    " greater_is_better=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "jlrZrFFiwpx8",
    "outputId": "f54fafe2-2de5-479d-c8e9-1582ae5c4f6a"
   },
   "outputs": [],
   "source": [
    "# Switch the model to training mode, enabling dropout etc layers.\n",
    "model.train()\n",
    "trainer = Trainer(\n",
    " model=model,\n",
    " args=training_args,\n",
    " train_dataset=training_set_enc,\n",
    " eval_dataset=validation_set_enc,\n",
    " processing_class=tokenizer,\n",
    " data_collator=DataCollatorForSeq2Seq(tokenizer),\n",
    " callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "KsDEyHXCxG9e",
    "outputId": "c4f167e8-2284-460e-a0b4-bc9d9b256212"
   },
   "outputs": [],
   "source": [
    "def display_evaluation(setname, results):\n",
    " print(f\"{setname} Set Loss:\", round(results[\"eval_loss\"], 3))\n",
    "\n",
    "# Switch the model to evaluation mode, disabling dropout etc layers.\n",
    "model.eval()\n",
    "# Evaluate the datasets.\n",
    "display_evaluation(\"Training\", trainer.evaluate(training_set_enc))\n",
    "display_evaluation(\"Testing\", trainer.evaluate(testing_set_enc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CJgy_eQfxOvZ"
   },
   "outputs": [],
   "source": [
    "from itertools import batched\n",
    "# Generates a response for a single input/question.\n",
    "def generate_response(tokenizer, model, question):\n",
    " # Convert the sentences into a list of numeric tokens. We instruct the tokenizer\n",
    " # to return PyTorch tensors (\"pt\") so that we can feed them directly into the model.\n",
    " tokenized = tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True,\n",
    " max_length=MAX_OUTPUT_LENGTH).to(model.device)\n",
    "\n",
    " # Generate outputs using the model.\n",
    " with torch.no_grad():\n",
    "  outputs = model.generate(**tokenized)\n",
    " # The model outputs a list of numeric tokens. To convert these tokens back to\n",
    " # sentences, we can use the batch_decode function from the tokenizer.\n",
    " outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    " return outputs\n",
    "\n",
    "# Generates a list of responses from the specified model, optionally including\n",
    "# the context in the prompt. If limit is set, then answers will only be generated\n",
    "# for the first N questions of the dataset.\n",
    "def generate_answers(tokenizer, model, dataset, use_context=True, limit=None):\n",
    " # Subsampling if requested.\n",
    " if limit is not None:\n",
    "  dataset = dataset.select(range(limit))\n",
    " # Create list of encoded tokens, similarly to how we preprocessed the data for\n",
    " # training. We do this so we can use batch processing to speed up inference.\n",
    " questions = []\n",
    " inputs = []\n",
    " references = []\n",
    " for sample in dataset:\n",
    "  question_with_context, question, answer = extract_sample_parts(sample)\n",
    "  # Only include the context if the caller requested it.\n",
    "  if use_context:\n",
    "    inputs.append(question_with_context)\n",
    "  else:\n",
    "    inputs.append(question)\n",
    "  # Include the original question/answer.\n",
    "  questions.append(question)\n",
    "  references.append(answer)\n",
    " # Generate responses for each of the prompts/inputs.\n",
    " # Submitting each question to the model separately would significantly\n",
    " # increase processing time, especially if the model is located on the GPU.\n",
    " # Instead, we group questions together in the same batch size that we used\n",
    " # for training.\n",
    " outputs = []\n",
    " for samples in batched(inputs, 128):\n",
    "  # Python's batched() function returns a tuple of the batch\n",
    "  # size, which we have to first convert to a list.\n",
    "  responses = generate_response(tokenizer, model, list(samples))\n",
    "  # generate_responses() returns an equal-sized list of responses.\n",
    "  outputs.extend(responses)\n",
    " # The length of the reference responses should equal the length of the\n",
    " # generated responses.\n",
    " assert (len(outputs) == len(references))\n",
    " return outputs, references, questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SY1wdn_ZxjCU"
   },
   "outputs": [],
   "source": [
    "answers_ctx, refs_ctx, questions_ctx = generate_answers(\n",
    " tokenizer, model, test_dataset, True, 100)\n",
    "answers_noctx, refs_noctx, questions_noctx = generate_answers(\n",
    " tokenizer, model, test_dataset, False, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_NAnBx6Ax9wN",
    "outputId": "01a0bec3-61a4-4664-e485-302f5dfa67b6"
   },
   "outputs": [],
   "source": [
    "def display_answer_and_references(question, answer, reference):\n",
    " print(\"Question:\", question)\n",
    " print(\"Generated answer:\", answer)\n",
    " print(\"Reference answer:\", reference)\n",
    " print()\n",
    "print(\"*** With context ***\")\n",
    "for i in range(3):\n",
    " display_answer_and_references(questions_ctx[i], answers_ctx[i],\n",
    " refs_ctx[i])\n",
    "print(\"*** Without context ***\")\n",
    "for i in range(3):\n",
    " display_answer_and_references(questions_noctx[i],\n",
    " answers_noctx[i], refs_noctx[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "loywDv91Q1_e"
   },
   "outputs": [],
   "source": [
    "# Computes the average score of a given metric from a list of ROUGE scores.\n",
    "def compute_average_score(scores, metric, key):\n",
    " total = 0\n",
    " for i in range(len(scores)):\n",
    "  # Since it's not a map, we have to manually read the attribute.\n",
    "  total += getattr(scores[i][metric], key)\n",
    " return total / len(scores)\n",
    " # Computes ROGUE-1, ROGUE-2 and ROGUE-L scores for the given generated\n",
    "# answers and reference answers.\n",
    "def compute_rouge(predictions, references):\n",
    " # Compute ROUGE-1, ROGUE-2 and ROUGE-L.\n",
    " metrics = [\"rouge1\", \"rouge2\", \"rougeL\"]\n",
    "\n",
    " # Use Porter stemmer to strip word suffixes to improve matching.\n",
    " scorer = rouge_scorer.RougeScorer(metrics, use_stemmer=True)\n",
    " # For each answer/reference pair, compute the ROUGE metrics.\n",
    " scores = []\n",
    " for prediction, reference in zip(predictions, references):\n",
    "  scores.append(scorer.score(reference, prediction))\n",
    " # Compute the average precision, recall and F1 score for each metric.\n",
    " results = {}\n",
    " for metric in metrics:\n",
    "  for k in [\"precision\", \"recall\", \"fmeasure\"]:\n",
    "    results[f\"{metric}_{k}\"] = compute_average_score(\n",
    "    scores, metric, k)\n",
    " return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P9LTTyPGRPzc",
    "outputId": "e4f3f385-5684-4c18-e5e2-a0e9aafd20b0"
   },
   "outputs": [],
   "source": [
    "print(\"ROUGE with context:\", compute_rouge(answers_ctx, refs_ctx))\n",
    "print()\n",
    "print(\"ROUGE without context:\", compute_rouge(answers_noctx, refs_noctx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tTtRLmgHRUkB",
    "outputId": "b8177519-452a-4a38-cdc1-ea3582d90d7b"
   },
   "outputs": [],
   "source": [
    "context = \"\"\"T5 is a series of large language models developed by Google AI introduced in\n",
    "2019.\n",
    "Like the original Transformer model, T5 models are encoder-decoder Transformers, where\n",
    "the encoder processes the input text, and\n",
    "the decoder generates the output text. T5 models are usually pretrained on a massive\n",
    "dataset of text and code, after which they can\n",
    "perform the text-based tasks that are similar to their pretrained tasks. They can also be\n",
    "finetuned to perform other tasks.\n",
    "T5 models have been employed in various applications, including chatbots, machine\n",
    "translation systems, text summarization tools,\n",
    "code generation, and robotics.\"\"\"\n",
    "question = \"What was T5 pretrained on?\"\n",
    "question_and_context = encode_question_and_context(question, context)\n",
    "print(\"Question:\", question)\n",
    "print(\"Answer:\", generate_response(tokenizer, model, [question_and_context]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ijpayLvzUn9v",
    "outputId": "9ec38ed1-f5bd-41ba-bdee-62dceae65a72"
   },
   "outputs": [],
   "source": [
    "samples = test_dataset.select(range(5))\n",
    "\n",
    "prompts_context = [\n",
    "    encode_question_and_context(s[\"question\"], s[\"context\"])\n",
    "    for s in samples\n",
    "]\n",
    "\n",
    "preds_context = generate_response(tokenizer, model, prompts_context)\n",
    "\n",
    "for i, s in enumerate(samples):\n",
    "    print(\"*With context*\")\n",
    "    print(\"Question:\", s[\"question\"])\n",
    "    print(\"Generated answer:\", preds_context[i])\n",
    "    print(\"Reference answer:\", s[\"answers\"][\"text\"][0])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sYsXdK7gVk8W",
    "outputId": "9170438a-eff9-4cad-ee43-aff8b2eff482"
   },
   "outputs": [],
   "source": [
    "samples = test_dataset.select(range(5))\n",
    "\n",
    "prompts_nocontext = [f\"question: {s['question']}\" for s in samples]\n",
    "preds_nocontext = generate_response(tokenizer, model, prompts_nocontext)\n",
    "\n",
    "for i, s in enumerate(samples):\n",
    "    print(\"*No context*\")\n",
    "    print(\"Question:\", s[\"question\"])\n",
    "    print(\"Generated answer:\", preds_nocontext[i])\n",
    "    print(\"Reference answer:\", s[\"answers\"][\"text\"][0])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zCgFLSvua_9I",
    "outputId": "3640d2e9-3adc-4bb5-fe42-8a7ad069dff3"
   },
   "outputs": [],
   "source": [
    "model_name = \"MaRiOrOsSi/t5-base-finetuned-question-answering\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "modefinetuned = AutoModelWithLMHead.from_pretrained(model_name)\n",
    "question = \"What is 42?\"\n",
    "context = \"42 is the answer to life, the universe and everything\"\n",
    "input = f\"question: {question} context: {context}\"\n",
    "encoded_input = tokenizer([input],\n",
    "                             return_tensors='pt',\n",
    "                             max_length=512,\n",
    "                             truncation=True)\n",
    "output = modefinetuned.generate(input_ids = encoded_input.input_ids,\n",
    "                            attention_mask = encoded_input.attention_mask)\n",
    "output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S71Aavn-cuhz"
   },
   "outputs": [],
   "source": [
    "answers_ctx, refs_ctx, questions_ctx = generate_answers(\n",
    " tokenizer, modefinetuned, test_dataset, True, 100)\n",
    "answers_noctx, refs_noctx, questions_noctx = generate_answers(\n",
    " tokenizer, modefinetuned, test_dataset, False, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RsXHVvIabUFL",
    "outputId": "232b29e8-ea34-48be-d9c2-3219ce9568ce"
   },
   "outputs": [],
   "source": [
    "print(\"ROUGE with context:\", compute_rouge(answers_ctx, refs_ctx))\n",
    "print()\n",
    "print(\"ROUGE without context:\", compute_rouge(answers_noctx, refs_noctx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4TOo0ZQAdxcG",
    "outputId": "f02611ca-b578-4ed5-8555-972520eb6b2c"
   },
   "outputs": [],
   "source": [
    "samples = test_dataset.select(range(5))\n",
    "\n",
    "prompts_context = [\n",
    "    encode_question_and_context(s[\"question\"], s[\"context\"])\n",
    "    for s in samples\n",
    "]\n",
    "\n",
    "preds_context = generate_response(tokenizer, modefinetuned, prompts_context)\n",
    "\n",
    "for i, s in enumerate(samples):\n",
    "    print(\"*With context*\")\n",
    "    print(\"Question:\", s[\"question\"])\n",
    "    print(\"Generated answer:\", preds_context[i])\n",
    "    print(\"Reference answer:\", s[\"answers\"][\"text\"][0])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EN5u1DiudyjE",
    "outputId": "6c4496bf-f7c4-4f2c-ce54-ac66e4333a91"
   },
   "outputs": [],
   "source": [
    "samples = test_dataset.select(range(5))\n",
    "\n",
    "prompts_nocontext = [f\"question: {s['question']}\" for s in samples]\n",
    "preds_nocontext = generate_response(tokenizer, modefinetuned, prompts_nocontext)\n",
    "\n",
    "for i, s in enumerate(samples):\n",
    "    print(\"*No context*\")\n",
    "    print(\"Question:\", s[\"question\"])\n",
    "    print(\"Generated answer:\", preds_nocontext[i])\n",
    "    print(\"Reference answer:\", s[\"answers\"][\"text\"][0])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x1FNnj6sz2aN",
    "outputId": "1dcf10f7-a300-4893-ce17-269acb14a162"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model_name = \"mrm8488/t5-base-finetuned-squadv2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "modelsQuADv2 = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "question = \"What is 42?\"\n",
    "context = \"42 is the answer to life, the universe and everything\"\n",
    "input = f\"question: {question} context: {context}\"\n",
    "encoded_input = tokenizer([input],\n",
    "                             return_tensors='pt',\n",
    "                             max_length=512,\n",
    "                             truncation=True)\n",
    "output = modelsQuADv2.generate(input_ids = encoded_input.input_ids,\n",
    "                            attention_mask = encoded_input.attention_mask)\n",
    "output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dZswrDrk0KTT"
   },
   "outputs": [],
   "source": [
    "answers_ctx, refs_ctx, questions_ctx = generate_answers(\n",
    " tokenizer, modelsQuADv2, test_dataset, True, 100)\n",
    "answers_noctx, refs_noctx, questions_noctx = generate_answers(\n",
    " tokenizer, modelsQuADv2, test_dataset, False, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K585-LCW0bNt",
    "outputId": "a193c340-6dfb-4551-ff80-09778ac3e458"
   },
   "outputs": [],
   "source": [
    "samples = test_dataset.select(range(5))\n",
    "\n",
    "prompts_context = [\n",
    "    encode_question_and_context(s[\"question\"], s[\"context\"])\n",
    "    for s in samples\n",
    "]\n",
    "\n",
    "preds_context = generate_response(tokenizer, modelsQuADv2, prompts_context)\n",
    "\n",
    "for i, s in enumerate(samples):\n",
    "    print(\"*With context*\")\n",
    "    print(\"Question:\", s[\"question\"])\n",
    "    print(\"Generated answer:\", preds_context[i])\n",
    "    print(\"Reference answer:\", s[\"answers\"][\"text\"][0])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CC3crRc70hsc",
    "outputId": "651ef50d-8bed-40fa-a85f-3ee230327813"
   },
   "outputs": [],
   "source": [
    "samples = test_dataset.select(range(5))\n",
    "\n",
    "prompts_nocontext = [f\"question: {s['question']}\" for s in samples]\n",
    "preds_nocontext = generate_response(tokenizer, modelsQuADv2, prompts_nocontext)\n",
    "\n",
    "for i, s in enumerate(samples):\n",
    "    print(\"*No context*\")\n",
    "    print(\"Question:\", s[\"question\"])\n",
    "    print(\"Generated answer:\", preds_nocontext[i])\n",
    "    print(\"Reference answer:\", s[\"answers\"][\"text\"][0])\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
